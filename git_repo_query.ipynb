{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import cohere\n",
    "import datetime\n",
    "import git\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import Image, Markdown, clear_output, display\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "from typing import Annotated, List, Literal, TypedDict\n",
    "\n",
    "from langchain.chains import RetrievalQA, create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.globals import set_debug, set_verbose\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_text_splitters import Language, MarkdownTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_core.messages import BaseMessage, ToolMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.document_loaders import GitLoader, generic\n",
    "from langchain_community.document_loaders.parsers import LanguageParser\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from fastembed import TextEmbedding\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "tavily_api_key = os.getenv(\"TAVILY_API_KEY\")\n",
    "langchain_api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "langchain_endpoint = os.getenv(\"LANGCHAIN_ENDPOINT\")\n",
    "os.environ[\"COHERE_API_KEY\"] = os.getenv(\"COHERE_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"classic_bot\"\n",
    "\n",
    "# Set global configurations\n",
    "set_verbose(False)\n",
    "set_debug(False)\n",
    "\n",
    "# Markdown display function\n",
    "def md(t):\n",
    "    display(Markdown(str(t)))\n",
    "\n",
    "llm = ChatGroq(temperature=1, model_name=\"llama3-70b-8192\", api_key=groq_api_key)\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest commit in https://github.com/microsoft/autogen: 84577570ad113ea1add2c3f60f47d4ddb75b98bb\n",
      "Latest commit in https://github.com/vcappuccio/achainflow: 673ba0eb5c2cfd538eb4a9edcb7a31b7f61c9131\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# List of repositories to clone\n",
    "repo_urls = [\n",
    "    \"https://github.com/microsoft/autogen\",\n",
    "    \"https://github.com/vcappuccio/achainflow\",\n",
    "    # Add more repository URLs here\n",
    "]\n",
    "\n",
    "# Base directory to store all cloned repositories\n",
    "base_repo_path = \"docs\"\n",
    "\n",
    "# Create the base directory if it doesn't exist\n",
    "if not os.path.exists(base_repo_path):\n",
    "    os.makedirs(base_repo_path)\n",
    "\n",
    "# Function to clone or update a repository\n",
    "def clone_or_update_repo(repo_url, base_path):\n",
    "    repo_name = repo_url.split('/')[-1]\n",
    "    repo_path = os.path.join(base_path, repo_name)\n",
    "    \n",
    "    # Clone or update the repository\n",
    "    if os.path.exists(repo_path):\n",
    "        repo = git.Repo(repo_path)\n",
    "        origin = repo.remotes.origin\n",
    "        origin.pull()\n",
    "    else:\n",
    "        repo = git.Repo.clone_from(repo_url, repo_path)\n",
    "        \n",
    "    return repo\n",
    "\n",
    "# Clone or update all repositories and print their latest commits\n",
    "for repo_url in repo_urls:\n",
    "    repo = clone_or_update_repo(repo_url, base_repo_path)\n",
    "    latest_commit = repo.head.commit\n",
    "    print(f\"Latest commit in {repo_url}: {latest_commit.hexsha}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "def ingest_docs(question, index_name, repo_path) -> str:\n",
    "    if not os.path.exists(index_name):\n",
    "        loader = GitLoader(repo_path=repo_path)\n",
    "        try:\n",
    "            raw_documents = loader.load()\n",
    "        except Exception as e:\n",
    "            md(f\"Error loading documents from the repository: {e}\")\n",
    "            return \"Error loading documents from the repository.\"\n",
    "        \n",
    "        if not raw_documents:\n",
    "            md(\"No documents found in the repository.\")\n",
    "            return \"No documents found in the repository.\"\n",
    "        \n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=20,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "        documents = text_splitter.split_documents(documents=raw_documents)\n",
    "\n",
    "        if not documents:\n",
    "            md(\"No documents to split.\")\n",
    "            return \"No documents to split.\"\n",
    "        \n",
    "        md(f\"Split into {len(documents)} chunks\")\n",
    "\n",
    "        embedding_model = OllamaEmbeddings()\n",
    "        \n",
    "        vectorstore = FAISS.from_documents(documents, embedding_model)\n",
    "        vectorstore.save_local(index_name)\n",
    "\n",
    "    md(\"Loading index...\")\n",
    "    embedding_model = OllamaEmbeddings()\n",
    "    my_vectorstore = FAISS.load_local(index_name, embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "    \n",
    "    qa = RetrievalQA.from_chain_type(llm=llm, retriever=my_vectorstore.as_retriever(), chain_type=\"stuff\")\n",
    "    response = qa.invoke({\"query\": question})\n",
    "    md(\"Done\")\n",
    "\n",
    "    return response[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Loading index..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Done"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "I don't know. The provided context does not mention AutoGen or multimodal agents. The context only talks about a Python script that interacts with Groq and Ollama LLMs to generate a final answer based on user input."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"\"\"How does AutoGen use multimodal agents?.\"\"\"\n",
    " \n",
    "index_name = \"faiss_achainflow\"\n",
    "repo_path = \"docs/achainflow\"\n",
    "\n",
    "response = ingest_docs(question = question, index_name = index_name, repo_path = repo_path)\n",
    "md(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_index.indices.base'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Settings, VectorStoreIndex\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfastembed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastEmbedEmbeddings\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseIndex\n\u001b[0;32m      5\u001b[0m BaseIndex\u001b[38;5;241m.\u001b[39mfrom_documents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mcls\u001b[39m, documents, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfrom_documents(documents, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Assuming the existence of necessary imports and classes for GitLoader, RecursiveCharacterTextSplitter, OllamaEmbeddings, and RetrievalQA\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'llama_index.indices.base'"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceBgeEmbeddings\n",
    "from llama_index.core import Settings, VectorStoreIndex\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "\n",
    "\n",
    "\n",
    "def ingest_docs(question, index_name, repo_path) -> str:\n",
    "    if not os.path.exists(index_name):\n",
    "        loader = GitLoader(repo_path=repo_path)\n",
    "        try:\n",
    "            raw_documents = loader.load()\n",
    "        except Exception as e:\n",
    "            md(f\"Error loading documents from the repository: {e}\")\n",
    "            return \"Error loading documents from the repository.\"\n",
    "        \n",
    "        if not raw_documents:\n",
    "            md(\"No documents found in the repository.\")\n",
    "            return \"No documents found in the repository.\"\n",
    "        \n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=20,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "        documents = text_splitter.split_documents(documents=raw_documents)\n",
    "\n",
    "        if not documents:\n",
    "            md(\"No documents to split.\")\n",
    "            return \"No documents to split.\"\n",
    "        \n",
    "        md(f\"Split into {len(documents)} chunks\")\n",
    "\n",
    "        embedding_model = FastEmbedEmbeddings(model_name=\"BAAI/bge-base-en\")\n",
    "        \n",
    "        vectorstore = FAISS.from_documents(documents, embedding_model)\n",
    "        vectorstore.save_local(index_name)\n",
    "\n",
    "    md(\"Loading index...\")\n",
    "    embedding_model = FastEmbedEmbeddings(model_name=\"BAAI/bge-base-en\")\n",
    "    my_vectorstore = FAISS.load_local(index_name, embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "    \n",
    "    qa = RetrievalQA.from_chain_type(llm=llm, retriever=my_vectorstore.as_retriever(), chain_type=\"stuff\")\n",
    "    response = qa.invoke({\"query\": question})\n",
    "    md(\"Done\")\n",
    "\n",
    "    return response[\"result\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 15576 chunks\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Document' object has no attribute 'get_doc_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m index_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfaiss_autogen\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     13\u001b[0m repo_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocs/autogen\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 15\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mingest_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m md(response)\n",
      "Cell \u001b[1;32mIn[83], line 36\u001b[0m, in \u001b[0;36mingest_docs\u001b[1;34m(question, index_name, repo_path)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo documents to split.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSplit into \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(documents)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m     vectorstore \u001b[38;5;241m=\u001b[39m \u001b[43mVectorStoreIndex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     vectorstore\u001b[38;5;241m.\u001b[39msave_local(index_name)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading index...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\redzh\\miniconda3\\envs\\teleprompt\\Lib\\site-packages\\llama_index\\core\\indices\\base.py:136\u001b[0m, in \u001b[0;36mBaseIndex.from_documents\u001b[1;34m(cls, documents, storage_context, show_progress, callback_manager, transformations, service_context, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m callback_manager\u001b[38;5;241m.\u001b[39mas_trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex_construction\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[1;32m--> 136\u001b[0m         docstore\u001b[38;5;241m.\u001b[39mset_document_hash(\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_doc_id\u001b[49m(), doc\u001b[38;5;241m.\u001b[39mhash)\n\u001b[0;32m    138\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m run_transformations(\n\u001b[0;32m    139\u001b[0m         documents,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    140\u001b[0m         transformations,\n\u001b[0;32m    141\u001b[0m         show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    143\u001b[0m     )\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[0;32m    146\u001b[0m         nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[0;32m    147\u001b[0m         storage_context\u001b[38;5;241m=\u001b[39mstorage_context,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    153\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Document' object has no attribute 'get_doc_id'"
     ]
    }
   ],
   "source": [
    " \n",
    "question = \"\"\"\n",
    "Write python code for 3 agents, 2 assistant agents and another being a user agent, the assistant agents\n",
    "will be a writer, and critic.  The user agent will just be simple. We also need to create a group chat and initiate\n",
    "it.  Create a config_list based on examples from other Autogen code examples. Have all the correct imports,\n",
    "the correct code for each agent, and initiate the group chat asking a message about the top 5 longest rivers in the\n",
    "world. Look at example code from AutoGen in order to understand how to do this if needed.  I don't want a simplified\n",
    "version, give me the full version.  Only return code, nothing else.  The agents should be AutoGen agents, not openai.\n",
    "Make sure to use the UserAgent and AssistantAgent, GroupChat and GroupChatManager agents to create the group chat and\n",
    "initiate the chat with.\n",
    "\"\"\"\n",
    "\n",
    "index_name = \"faiss_autogen\"\n",
    "repo_path = \"docs/autogen\"\n",
    "\n",
    "response = ingest_docs(question=question, index_name=index_name, repo_path=repo_path)\n",
    "md(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teleprompt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
