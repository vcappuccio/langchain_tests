{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Git Ripo Ruunner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import cohere\n",
    "import datetime\n",
    "import git\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import Annotated, List, Literal, TypedDict\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import Image, Markdown, clear_output, display\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "\n",
    "from fastembed import TextEmbedding\n",
    "from langchain.chains import RetrievalQA, create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.embeddings.huggingface import HuggingFaceBgeEmbeddings\n",
    "from langchain.globals import set_debug, set_verbose\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import GitLoader, generic\n",
    "from langchain_community.document_loaders.parsers import LanguageParser\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.messages import BaseMessage, ToolMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_text_splitters import Language, MarkdownTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from llama_index.core import Settings, VectorStoreIndex\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "tavily_api_key = os.getenv(\"TAVILY_API_KEY\")\n",
    "langchain_api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "langchain_endpoint = os.getenv(\"LANGCHAIN_ENDPOINT\")\n",
    "os.environ[\"COHERE_API_KEY\"] = os.getenv(\"COHERE_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"classic_bot\"\n",
    "\n",
    "# Set global configurations\n",
    "set_verbose(False)\n",
    "set_debug(False)\n",
    "\n",
    "# Markdown display function\n",
    "def md(t):\n",
    "    display(Markdown(str(t)))\n",
    "\n",
    "llm = ChatGroq(temperature=1, model_name=\"llama3-70b-8192\", api_key=groq_api_key)\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest commit in https://github.com/microsoft/autogen: 84577570ad113ea1add2c3f60f47d4ddb75b98bb\n",
      "Latest commit in https://github.com/vcappuccio/achainflow: 673ba0eb5c2cfd538eb4a9edcb7a31b7f61c9131\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# List of repositories to clone\n",
    "repo_urls = [\n",
    "    \"https://github.com/microsoft/autogen\",\n",
    "    \"https://github.com/vcappuccio/achainflow\",\n",
    "    # Add more repository URLs here\n",
    "]\n",
    "\n",
    "# Base directory to store all cloned repositories\n",
    "base_repo_path = \"docs\"\n",
    "\n",
    "# Create the base directory if it doesn't exist\n",
    "if not os.path.exists(base_repo_path):\n",
    "    os.makedirs(base_repo_path)\n",
    "\n",
    "# Function to clone or update a repository\n",
    "def clone_or_update_repo(repo_url, base_path):\n",
    "    repo_name = repo_url.split('/')[-1]\n",
    "    repo_path = os.path.join(base_path, repo_name)\n",
    "    \n",
    "    # Clone or update the repository\n",
    "    if os.path.exists(repo_path):\n",
    "        repo = git.Repo(repo_path)\n",
    "        origin = repo.remotes.origin\n",
    "        origin.pull()\n",
    "    else:\n",
    "        repo = git.Repo.clone_from(repo_url, repo_path)\n",
    "        \n",
    "    return repo\n",
    "\n",
    "# Clone or update all repositories and print their latest commits\n",
    "for repo_url in repo_urls:\n",
    "    repo = clone_or_update_repo(repo_url, base_repo_path)\n",
    "    latest_commit = repo.head.commit\n",
    "    print(f\"Latest commit in {repo_url}: {latest_commit.hexsha}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ingest_docs(question, index_name, repo_path) -> str:\n",
    "    if not os.path.exists(index_name):\n",
    "        loader = GitLoader(repo_path=repo_path)\n",
    "        try:\n",
    "            raw_documents = loader.load()\n",
    "        except Exception as e:\n",
    "            md(f\"Error loading documents from the repository: {e}\")\n",
    "            return \"Error loading documents from the repository.\"\n",
    "        \n",
    "        if not raw_documents:\n",
    "            md(\"No documents found in the repository.\")\n",
    "            return \"No documents found in the repository.\"\n",
    "        \n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=20,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "        documents = text_splitter.split_documents(documents=raw_documents)\n",
    "\n",
    "        if not documents:\n",
    "            md(\"No documents to split.\")\n",
    "            return \"No documents to split.\"\n",
    "        \n",
    "        md(f\"Split into {len(documents)} chunks\")\n",
    "\n",
    "        embedding_model = OllamaEmbeddings()\n",
    "        \n",
    "        vectorstore = FAISS.from_documents(documents, embedding_model)\n",
    "        vectorstore.save_local(index_name)\n",
    "\n",
    "    md(\"Loading index...\")\n",
    "    embedding_model = OllamaEmbeddings()\n",
    "    my_vectorstore = FAISS.load_local(index_name, embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "    \n",
    "    qa = RetrievalQA.from_chain_type(llm=llm, retriever=my_vectorstore.as_retriever(), chain_type=\"stuff\")\n",
    "    response = qa.invoke({\"query\": question})\n",
    "    md(\"Done\")\n",
    "\n",
    "    return response[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Loading index..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Done"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "This repo is about a Python script called `achainflow.py` that provides a Streamlit application to consult a chain of advisors using different AI models (Groq and Ollama) to solve coding problems and technical questions. It takes a user's problem statement as input, consults multiple AI models asynchronously, and generates a final answer based on the responses from the models."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"\"\"What is this repo about?\"\"\"\n",
    " \n",
    "index_name = \"faiss_achainflow\"\n",
    "repo_path = \"docs/achainflow\"\n",
    "\n",
    "response = ingest_docs(question = question, index_name = index_name, repo_path = repo_path)\n",
    "md(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using FastEmbed Embeddings\n",
    "\n",
    "\n",
    "def ingest_docs(question, index_name, repo_path) -> str:\n",
    "    if not os.path.exists(index_name):\n",
    "        loader = GitLoader(repo_path=repo_path)\n",
    "        try:\n",
    "            raw_documents = loader.load()\n",
    "        except Exception as e:\n",
    "            md(f\"Error loading documents from the repository: {e}\")\n",
    "            return \"Error loading documents from the repository.\"\n",
    "        \n",
    "        if not raw_documents:\n",
    "            md(\"No documents found in the repository.\")\n",
    "            return \"No documents found in the repository.\"\n",
    "        \n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=20,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "        documents = text_splitter.split_documents(documents=raw_documents)\n",
    "\n",
    "        if not documents:\n",
    "            md(\"No documents to split.\")\n",
    "            return \"No documents to split.\"\n",
    "        \n",
    "        md(f\"Split into {len(documents)} chunks\")\n",
    "\n",
    "        embedding_model = FastEmbedEmbeddings(model_name=\"BAAI/bge-base-en\")\n",
    "        \n",
    "        vectorstore = FAISS.from_documents(documents, embedding_model)\n",
    "        vectorstore.save_local(index_name)\n",
    "\n",
    "    md(\"Loading index...\")\n",
    "    embedding_model = FastEmbedEmbeddings(model_name=\"BAAI/bge-base-en\")\n",
    "    my_vectorstore = FAISS.load_local(index_name, embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "    \n",
    "    qa = RetrievalQA.from_chain_type(llm=llm, retriever=my_vectorstore.as_retriever(), chain_type=\"stuff\")\n",
    "    response = qa.invoke({\"query\": question})\n",
    "    md(\"Done\")\n",
    "\n",
    "    return response[\"result\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Loading index..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Done"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "import os\n",
       "from autogen.agent import AssistantAgent, UserAgent, GroupChat, GroupChatManager\n",
       "\n",
       "# Define LLM config\n",
       "config_list = [{\"model\": \"gpt-3.5-turbo\", \"api_key\": os.getenv(\"OPENAI_API_KEY\")}]\n",
       "\n",
       "# Create agents\n",
       "writer = AssistantAgent(\"writer\", llm_config={\"config_list\": config_list})\n",
       "critic = AssistantAgent(\"critic\", llm_config={\"config_list\": config_list})\n",
       "user = UserAgent(\"user\")\n",
       "\n",
       "# Create group chat\n",
       "group_chat = GroupChat(\"group_chat\")\n",
       "group_chat.add_agent(writer)\n",
       "group_chat.add_agent(critic)\n",
       "group_chat.add_agent(user)\n",
       "\n",
       "# Create group chat manager\n",
       "group_chat_manager = GroupChatManager()\n",
       "\n",
       "# Initiate group chat\n",
       "group_chat_manager.initiate_group_chat(group_chat, message=\"What are the top 5 longest rivers in the world?\")\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " \n",
    "question = \"\"\"\n",
    "Write python code for 3 agents, 2 assistant agents and another being a user agent, the assistant agents\n",
    "will be a writer, and critic.  The user agent will just be simple. We also need to create a group chat and initiate\n",
    "it.  Create a config_list based on examples from other Autogen code examples. Have all the correct imports,\n",
    "the correct code for each agent, and initiate the group chat asking a message about the top 5 longest rivers in the\n",
    "world. Look at example code from AutoGen in order to understand how to do this if needed.  I don't want a simplified\n",
    "version, give me the full version.  Only return code, nothing else.  The agents should be AutoGen agents, not openai.\n",
    "Make sure to use the UserAgent and AssistantAgent, GroupChat and GroupChatManager agents to create the group chat and\n",
    "initiate the chat with.\n",
    "\"\"\"\n",
    "\n",
    "index_name = \"faiss_autogen\"\n",
    "repo_path = \"docs/autogen\"\n",
    "\n",
    "response = ingest_docs(question=question, index_name=index_name, repo_path=repo_path)\n",
    "md(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teleprompt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
